"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
const types_1 = require("./types");
const audioworklet_1 = __importDefault(require("./audioworklet"));
const audioProcessEvent = 'audioprocess';
const baseBufferSize = 4096;
class BrowserMicrophone {
    constructor(isWebkit, sampleRate, apiClient, debug = false) {
        this.initialized = false;
        this.muted = false;
        this.handleAudio = (array) => {
            if (this.muted) {
                return;
            }
            if (array.length > 0) {
                this.apiClient.sendAudio(array);
            }
        };
        this.isWebkit = isWebkit;
        this.apiClient = apiClient;
        this.sampleRate = sampleRate;
        this.debug = debug;
    }
    initialize(audioContext, opts) {
        var _a;
        return __awaiter(this, void 0, void 0, function* () {
            if (((_a = window.navigator) === null || _a === void 0 ? void 0 : _a.mediaDevices) === undefined) {
                throw types_1.ErrDeviceNotSupported;
            }
            this.audioContext = audioContext;
            this.resampleRatio = this.audioContext.sampleRate / this.sampleRate;
            try {
                this.mediaStream = yield window.navigator.mediaDevices.getUserMedia(opts);
            }
            catch (_b) {
                throw types_1.ErrNoAudioConsent;
            }
            this.audioTrack = this.mediaStream.getAudioTracks()[0];
            // Start audio context if we are dealing with a non-WebKit browser.
            //
            // Non-webkit browsers (currently only Chrome on Android)
            // require that user media is obtained before resuming the audio context.
            //
            // If audio context is attempted to be resumed before `mediaDevices.getUserMedia`,
            // `audioContext.resume()` will hang indefinitely, without being resolved or rejected.
            if (!this.isWebkit) {
                yield this.audioContext.resume();
            }
            if (window.AudioWorkletNode !== undefined) {
                const blob = new Blob([audioworklet_1.default], { type: 'text/javascript' });
                const blobURL = window.URL.createObjectURL(blob);
                yield this.audioContext.audioWorklet.addModule(blobURL);
                const speechlyNode = new AudioWorkletNode(this.audioContext, 'speechly-worklet');
                this.audioContext.createMediaStreamSource(this.mediaStream).connect(speechlyNode);
                speechlyNode.connect(this.audioContext.destination);
                if (window.SharedArrayBuffer !== undefined) {
                    // Chrome, Edge, Firefox, Firefox Android
                    const controlSAB = new window.SharedArrayBuffer(4 * Int32Array.BYTES_PER_ELEMENT);
                    const dataSAB = new window.SharedArrayBuffer(1024 * Float32Array.BYTES_PER_ELEMENT);
                    this.apiClient.postMessage({
                        type: 'SET_SHARED_ARRAY_BUFFERS',
                        controlSAB,
                        dataSAB,
                    });
                    speechlyNode.port.postMessage({
                        type: 'SET_SHARED_ARRAY_BUFFERS',
                        controlSAB,
                        dataSAB,
                    });
                }
                else {
                    if (this.debug) {
                        console.log('[SpeechlyClient]', 'can not use SharedArrayBuffer');
                    }
                    // Opera, Chrome Android, Webview Anroid
                    speechlyNode.port.onmessage = (event) => {
                        this.handleAudio(event.data);
                    };
                }
            }
            else {
                if (this.debug) {
                    console.log('[SpeechlyClient]', 'can not use AudioWorkletNode');
                }
                // Safari, iOS Safari and Internet Explorer
                if (this.isWebkit) {
                    // Multiply base buffer size of 4 kB by the resample ratio rounded up to the next power of 2.
                    // i.e. for 48 kHz to 16 kHz downsampling, this will be 4096 (base) * 4 = 16384.
                    const bufSize = baseBufferSize * Math.pow(2, Math.ceil(Math.log(this.resampleRatio) / Math.log(2)));
                    this.audioProcessor = this.audioContext.createScriptProcessor(bufSize, 1, 1);
                }
                else {
                    this.audioProcessor = this.audioContext.createScriptProcessor(undefined, 1, 1);
                }
                this.audioContext.createMediaStreamSource(this.mediaStream).connect(this.audioProcessor);
                this.audioProcessor.connect(this.audioContext.destination);
                this.audioProcessor.addEventListener(audioProcessEvent, (event) => {
                    this.handleAudio(event.inputBuffer.getChannelData(0));
                });
            }
            this.initialized = true;
            this.mute();
        });
    }
    close() {
        return __awaiter(this, void 0, void 0, function* () {
            this.mute();
            if (!this.initialized) {
                throw types_1.ErrNotInitialized;
            }
            const t = this.audioTrack;
            t.enabled = false;
            // Stop all media tracks
            const stream = this.mediaStream;
            stream.getTracks().forEach(t => t.stop());
            // Disconnect and stop ScriptProcessorNode
            if (this.audioProcessor != null) {
                const proc = this.audioProcessor;
                proc.disconnect();
            }
            // Unset all audio infrastructure
            this.mediaStream = undefined;
            this.audioTrack = undefined;
            this.audioProcessor = undefined;
            this.initialized = false;
        });
    }
    mute() {
        this.muted = true;
    }
    unmute() {
        this.muted = false;
    }
}
exports.BrowserMicrophone = BrowserMicrophone;
//# sourceMappingURL=browser_microphone.js.map